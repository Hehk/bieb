{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_book(path):\n",
    "  with open(path, 'r') as f:\n",
    "    return f.read().strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the book into paragraphs\n",
    "def parse_book(book):\n",
    "  return [line.strip().replace('\\n', ' ') for line in book.split('\\n\\n') if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_dick = parse_book(load_book('../books/2701.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(paragraphs):\n",
    "  encoded_input = tokenizer(paragraphs, padding=True, truncation=True, return_tensors='pt')\n",
    "  with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "  return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished chunk 0 of 29\n",
      "Finished chunk 1 of 29\n",
      "Finished chunk 2 of 29\n",
      "Finished chunk 3 of 29\n",
      "Finished chunk 4 of 29\n",
      "Finished chunk 5 of 29\n",
      "Finished chunk 6 of 29\n",
      "Finished chunk 7 of 29\n",
      "Finished chunk 8 of 29\n",
      "Finished chunk 9 of 29\n",
      "Finished chunk 10 of 29\n",
      "Finished chunk 11 of 29\n",
      "Finished chunk 12 of 29\n",
      "Finished chunk 13 of 29\n",
      "Finished chunk 14 of 29\n",
      "Finished chunk 15 of 29\n",
      "Finished chunk 16 of 29\n",
      "Finished chunk 17 of 29\n",
      "Finished chunk 18 of 29\n",
      "Finished chunk 19 of 29\n",
      "Finished chunk 20 of 29\n",
      "Finished chunk 21 of 29\n",
      "Finished chunk 22 of 29\n",
      "Finished chunk 23 of 29\n",
      "Finished chunk 24 of 29\n",
      "Finished chunk 25 of 29\n",
      "Finished chunk 26 of 29\n",
      "Finished chunk 27 of 29\n",
      "Finished chunk 28 of 29\n"
     ]
    }
   ],
   "source": [
    "embeddings = torch.zeros((0, 384))\n",
    "def chunk(list, chunk_n):\n",
    "    return [list[i:i+chunk_n] for i in range(0, len(list), chunk_n)]\n",
    "\n",
    "chunks = chunk(moby_dick, 100)\n",
    "for i, chunk in enumerate(chunks):\n",
    "  chunk_embeddings = generate_embeddings(chunk)\n",
    "  embeddings = torch.cat((embeddings, chunk_embeddings))\n",
    "  print(f'Finished chunk {i} of {len(chunks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, books):\n",
    "  query_embeddings = generate_embeddings([query])\n",
    "  results = []\n",
    "  books_embedding = torch.stack([book['embeddings'] for book in books])\n",
    "  distances = torch.nn.functional.cosine_similarity(query_embeddings, books_embedding, dim=2)\n",
    "  top_distances = torch.topk(torch.flatten(distances), k=10).indices\n",
    "  return [books[i // books_embedding.shape[1]][\"content\"][i % books_embedding.shape[1]] for i in top_distances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_book(name, path):\n",
    "  content = parse_book(load_book(path))\n",
    "  return {\n",
    "    'name': name,\n",
    "    'content': content,\n",
    "    'embeddings': embeddings\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2802, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/rkh4v1215fxgb7zdvtrqs8lr0000gn/T/ipykernel_36957/4135684763.py:8: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return [books[i // books_embedding.shape[1]][\"content\"][i % books_embedding.shape[1]] for i in top_distances]\n"
     ]
    }
   ],
   "source": [
    "X = search(\"How do you hunt a whale?\", [create_book(\"moby_dick_1\", '../books/2701.txt'), create_book(\"moby_dick_1\", '../books/2701.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*The ancient whale-cry upon first sighting a whale from the mast-head, still used by whalemen in hunting the famous Gallipagos terrapin.',\n",
       " '*The ancient whale-cry upon first sighting a whale from the mast-head, still used by whalemen in hunting the famous Gallipagos terrapin.',\n",
       " 'CHAPTER 103. Measurement of The Whale’s Skeleton.',\n",
       " 'CHAPTER 103. Measurement of The Whale’s Skeleton.',\n",
       " 'CHAPTER 103. Measurement of The Whale’s Skeleton.',\n",
       " 'CHAPTER 103. Measurement of The Whale’s Skeleton.',\n",
       " 'How vain and foolish, then, thought I, for timid untravelled man to try to comprehend aright this wondrous whale, by merely poring over his dead attenuated skeleton, stretched in this peaceful wood. No. Only in the heart of quickest perils; only when within the eddyings of his angry flukes; only on the profound unbounded sea, can the fully invested whale be truly and livingly found out.',\n",
       " 'How vain and foolish, then, thought I, for timid untravelled man to try to comprehend aright this wondrous whale, by merely poring over his dead attenuated skeleton, stretched in this peaceful wood. No. Only in the heart of quickest perils; only when within the eddyings of his angry flukes; only on the profound unbounded sea, can the fully invested whale be truly and livingly found out.',\n",
       " '“_What_ whale?”',\n",
       " '“_What_ whale?”']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda7e54fe21129b67f77862937907ee926f057597a3e2fa1e18ac955e40912b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
